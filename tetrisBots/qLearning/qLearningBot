import copy, main, os, time, math, random
from copy import deepcopy
import numpy as np



def get_column_heights(board):
    heights = []
    num_rows = len(board)
    num_cols = len(board[0])
    for col in range(num_cols):
        col_height = 0
        for row in range(num_rows):
            if board[row][col] != 0:
                col_height = num_rows - row
                break
        heights.append(col_height)
    return heights

def checkForRowsBumpiness(board):
    heights = get_column_heights(board)
    bumpiness = sum(abs(heights[i] - heights[i+1]) for i in range(len(heights) - 1))
    max_bumpiness = (len(board[0]) - 4) * len(board)
    return bumpiness / max_bumpiness

def findHolesInBoard(board):
    holes = 0
    for x in range(len(board[0])):
        foundHole = False
        for y in range(len(board)):
            positionValue = board[y][x]
            if positionValue == 1 or y <= 0:
                continue
            upValue = board[y-1][x]
            if upValue == 1 or foundHole:
                foundHole = True
                holes += 1
    return holes / (len(board) * (len(board[0]) - 4))

def findAggHeight(board):
    heights = get_column_heights(board)
    return sum(heights) / (len(board) * (len(board[0]) - 4))

def calculatePoints(tetrisBoard : main.TetrisBoard, w1, w2, w3):
    board = tetrisBoard.matrix
    heights = get_column_heights(board)
    holes = findHolesInBoard(board)
    avgh = sum(heights)
    bumpiness = checkForRowsBumpiness(board)
    points = 0
    points -= holes * w1
    points -= bumpiness * w3
    points -= avgh * w2
    return points

class QModel():
    def __init__(self, actions = ['n', 'a', 'd', 'r', 'q', 'x']):
        self.actions = actions
        self.memory = {}

    def __call__(self, state, action):
        key = (state, action)
        if key not in self.memory:
            self.memory[key] = 0.0
        return self.memory[key]

    def train(self, state, action, next_state, reward):
        old_q = self(state, action)
        # old_q = max(old_q)
        # print(old_q)
        future_q = [self(next_state, a) for a in self.actions]
        # print(future_q)
        future_q = max(future_q)
        # print(future_q)
        # print(reward)
        # input()
        new_q = old_q + 0.1 * (reward + 0.9 * future_q - old_q)
        self.memory[(state, action)] = new_q
    
        

        

class trainingScene():
    def __init__(self):
        self.board = main.TetrisBoard(8, 20)
        self.hasEnded = False
        self.actions = ['n', 'a', 'd', 'r', 'q', 'x']
        self.qModel = QModel(self.actions)
        self.totalSteps = 0
        # self.linesClearedByPoints = {0:0, 40:1, 100:2, 300:3, 1200:4}

    def resetBoard(self):
        self.board = main.TetrisBoard(8, 20)
        self.totalSteps = 0

    def discretize(self, value, bins):
        return round(value * bins)

    def runGame(self, showBard=False, boardText='', on_update=None):
        start = time.time()
        lost = False
        print('start')

        while not lost:
            self.totalSteps += 1
            if len(self.board.activePieces) <= 0:
                lost = self.board.step('n')
                continue

            heights = get_column_heights(self.board.matrix)

            holes = findHolesInBoard(self.board.matrix)
            avgh = sum(heights)
            bumpiness = checkForRowsBumpiness(self.board.matrix)
            points = self.board.clearedLines

            activePiece : main.Piece = self.board.activePieces[0]

            piece_type = type(activePiece)
            piece_index = next(i for i, cls in enumerate(main.pieces) if cls == piece_type)

            piecePos = activePiece.position
            px = piecePos[0]
            py = piecePos[1]

            pieceOrientation = activePiece.formIndex

            state = (
                    self.discretize(holes, 10),
                    self.discretize(avgh, 10),
                    self.discretize(bumpiness, 10),
                    piece_index,
                    pieceOrientation,
                    px,
                    py
                )
            
            epsilon = 0.1  # exploration rate

            if random.random() < epsilon:
                action = random.choice(self.actions)
            else:
                estimatedRewards = [self.qModel(state, a) for a in self.actions]
                index = estimatedRewards.index(max(estimatedRewards))
                action = self.actions[index]

            lost = self.board.step(action)

            newHoles = findHolesInBoard(self.board.matrix)
            newAvgh = sum(heights)
            newBumpiness = checkForRowsBumpiness(self.board.matrix)
            newPoints = self.board.clearedLines

            activePiece : main.Piece = self.board.activePieces[0]

            piece_type = type(activePiece)
            piece_index = next(i for i, cls in enumerate(main.pieces) if cls == piece_type)

            piecePos = activePiece.position
            px = piecePos[0]
            py = piecePos[1]

            pieceOrientation = activePiece.formIndex

            nextState = (
                    self.discretize(newHoles, 10),
                    self.discretize(newAvgh, 10),
                    self.discretize(newBumpiness, 10),
                    piece_index,
                    pieceOrientation,
                    px,
                    py
                )

            reward = 10 * (newPoints - points)
            reward -= 2 * (newHoles)
            reward -= (newAvgh)
            reward -= (newBumpiness)
            
            self.qModel.train(state, action, nextState, reward)

            if on_update:
                on_update()
            if showBard:
                os.system('clear')
                print(boardText)
                print(f"""
-----------------------------
POINTS: {self.board.points}
-----------------------------
NEXT PIECE: {self.board.nextPiece}
{self.board.activePieces}
""")
                self.board.renderPieces()
                print(action)
                holesNumber = findHolesInBoard(self.board.matrix)
                avg = findAggHeight(self.board.matrix)
                bumpiness = checkForRowsBumpiness(self.board.matrix)
                # print('Weights: ', self.weights)
                print('Holes: ', holesNumber)
                print('Height: ', avg)
                print('Bumpiness: ', bumpiness)
                print()
                print('Reward: ', reward)
                for i in range(len(self.actions)):
                    print('   ', self.actions[i], ':', estimatedRewards[i])
                print(len(self.qModel.memory))
                time.sleep(0.01)
        end = time.time()
        print(f"finished Game : \nholes = {findHolesInBoard(self.board.matrix)}\ntime duration = {end-start:.2f}s\nSteps = {self.totalSteps}\n")
        return self.board.points

def trainGenerations(gens=5000000):
    trsc = trainingScene()
    end = 0
    for i in range(gens):
        if i < end:
            _ = trsc.runGame(False, f'GENERATION {i}')
            trsc.resetBoard()
            continue
        
        _ = trsc.runGame(True, f'GENERATION {i}')
        j = input('??')
        if j == 'a':
            end = i + 5000
        if j == 's':
            end = i + 10000
        time.sleep(0.01)
        trsc.resetBoard()
        # input('continue?')

if __name__ == "__main__":
    trainGenerations()